{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP and Bellman Equation\n",
    "Agent environment interface: At each step t agent interact with environment at a given state s_t it perform action a_t and receive new state s_t+1 and reward r_t. Action choose by agent is based on policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total reward G_t is the sum of all reward starting from time t. future reward are discounted at the discount rate gamma^k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Property: The environment response at time t+1 depend only the state and action at time t, Means according to markov \n",
    "property the future is based on the present only not the past action.\n",
    "Markov is the environment for agent in reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Process(MDP): Defined by a action set A, state set S, and one step dynamic P(s', r / s, a) . If we know complete knowledge of environment we know the transition dynamic. In practice we often don't know the full MDP(but some knowledge of mdp). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value function V(s): It define how good a state is. after taking steps we calculate the value of that particular state and based on that know how good the state is. More formally it is expected return G_t given that the agent is in the state s.\n",
    "V(s) = Ex(G_t | S_t = s). value function is specific to a given policy pi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Value function: q(s,a) estimate how good it is. In a particular state agent choose action based on it how good it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman equation express the relationship between value function of current state and value fuction of successor state. It is also applicable for the action value function. It can be expressed by backup diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value function define an ordering over policy. A policy p1 is better than p2 if v_p1(s) >= v_p2(s) for all state s.\n",
    "for MDP there exist one or more optimal policy that are better than or equal to all other policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal state value function v*(s) is define by the value function for optimal policy. Same for q*(s,a). The bellman optimality equation how optimal value of a state is related to optimal value of successor state. It has max instead of an average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
