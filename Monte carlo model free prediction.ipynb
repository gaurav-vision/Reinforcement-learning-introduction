{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning goal\n",
    "\n",
    "Understand the difference between prediction and control\n",
    "\n",
    "Know how to use MC to predict state values and state action values\n",
    "\n",
    "Understand on policy first visit MC control algorithm\n",
    "\n",
    "Understand off policy first visit MC control algorithm\n",
    "\n",
    "Understand weighted importance Sampling\n",
    "\n",
    "Understand the benitfit of MC over DP approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The dynamic programming assume complete knowledge of environment(The MDP). In practice we often don't have full knowledge of the how the world works.\n",
    "\n",
    "Monte carlo learn directly from experience when it iteract with environment. An episode of experience is a series of (state action, reward, next state) tuple.\n",
    "\n",
    "MC method works based on episodes. We sample episode of experience and make update our estimate at the end of the each episode.  MC method have high variance(due to lots of random decisions within an episode) but are unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC policy evaluation: Given a policy we want to estimates state value function V(s). Sample episode of experience and estimate V(S) to be the reward received from that onword average across all your experience. the same technique works for action value function Q(s,a). Given enough samples, this is proven to converge.\n",
    "\n",
    "MC control: Idea is the same as dynamic programming. Use MC policy evaluation to evaluate the current policy then improve the policy greedly. The problem how we ensure that we explore all states if we don't know the full knowledge of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to exploration problem: Use epsilon greedy policy instead of using full greedy policy. When making a decision act randomly with probability epsilon. This will learn the optimal epsilon greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off policy learning: How can we learn the actual optimal (greedy policy) while following an exploratory (epsilon greedy) policy. We can use importance sampling, which weigh returns by there probability of occuring under the policy we want to learn about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dynamic programming we assume that we have full knowledge of environment (MDP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
