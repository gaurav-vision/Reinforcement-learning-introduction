{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a relevent variable\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "#Get the environment and extract the no of action available in cart pole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#build a very simple hidden layer neural network model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "   79/5000: episode: 1, duration: 10.835s, episode steps: 79, steps per second: 7, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.402, 0.722], loss: 0.428329, mean_absolute_error: 0.496236, mean_q: 0.052405\n",
      "  113/5000: episode: 2, duration: 0.538s, episode steps: 34, steps per second: 63, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.151 [-0.159, 0.753], loss: 0.353307, mean_absolute_error: 0.446486, mean_q: 0.191942\n",
      "  163/5000: episode: 3, duration: 0.833s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.082 [-0.295, 0.778], loss: 0.317570, mean_absolute_error: 0.468236, mean_q: 0.317697\n",
      "  197/5000: episode: 4, duration: 0.566s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.081 [-0.228, 0.770], loss: 0.275707, mean_absolute_error: 0.501667, mean_q: 0.471338\n",
      "  264/5000: episode: 5, duration: 1.118s, episode steps: 67, steps per second: 60, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.007 [-0.394, 0.770], loss: 0.233591, mean_absolute_error: 0.568451, mean_q: 0.685542\n",
      "  299/5000: episode: 6, duration: 0.600s, episode steps: 35, steps per second: 58, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.095 [-0.204, 0.813], loss: 0.170390, mean_absolute_error: 0.643666, mean_q: 0.961311\n",
      "  335/5000: episode: 7, duration: 0.615s, episode steps: 36, steps per second: 59, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.348, 0.951], loss: 0.140610, mean_absolute_error: 0.722506, mean_q: 1.176570\n",
      "  361/5000: episode: 8, duration: 0.435s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.396, 0.924], loss: 0.113920, mean_absolute_error: 0.806351, mean_q: 1.393671\n",
      "  383/5000: episode: 9, duration: 0.364s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-0.350, 0.881], loss: 0.101748, mean_absolute_error: 0.881627, mean_q: 1.581770\n",
      "  401/5000: episode: 10, duration: 0.301s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.413, 0.865], loss: 0.088448, mean_absolute_error: 0.932244, mean_q: 1.744954\n",
      "  423/5000: episode: 11, duration: 0.368s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.616, 0.942], loss: 0.096653, mean_absolute_error: 1.019635, mean_q: 1.924898\n",
      "  443/5000: episode: 12, duration: 0.329s, episode steps: 20, steps per second: 61, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.087 [-0.558, 1.174], loss: 0.086568, mean_absolute_error: 1.089483, mean_q: 2.082726\n",
      "  468/5000: episode: 13, duration: 0.416s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.080 [-0.369, 1.017], loss: 0.087109, mean_absolute_error: 1.178007, mean_q: 2.281801\n",
      "  490/5000: episode: 14, duration: 0.370s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.081 [-0.578, 1.094], loss: 0.095302, mean_absolute_error: 1.272387, mean_q: 2.476176\n",
      "  503/5000: episode: 15, duration: 0.214s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.093 [-0.792, 1.298], loss: 0.108810, mean_absolute_error: 1.370855, mean_q: 2.635041\n",
      "  515/5000: episode: 16, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.779, 1.336], loss: 0.129727, mean_absolute_error: 1.390702, mean_q: 2.704689\n",
      "  534/5000: episode: 17, duration: 0.316s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.072 [-0.587, 1.299], loss: 0.126167, mean_absolute_error: 1.482930, mean_q: 2.874557\n",
      "  552/5000: episode: 18, duration: 0.298s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.081 [-0.610, 1.197], loss: 0.172500, mean_absolute_error: 1.588331, mean_q: 3.056943\n",
      "  572/5000: episode: 19, duration: 0.334s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.097 [-0.772, 1.565], loss: 0.151780, mean_absolute_error: 1.648078, mean_q: 3.208601\n",
      "  583/5000: episode: 20, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.115 [-0.784, 1.384], loss: 0.097945, mean_absolute_error: 1.699346, mean_q: 3.395140\n",
      "  593/5000: episode: 21, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.146 [-0.771, 1.498], loss: 0.191785, mean_absolute_error: 1.765684, mean_q: 3.485171\n",
      "  605/5000: episode: 22, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-0.805, 1.493], loss: 0.128760, mean_absolute_error: 1.795997, mean_q: 3.586714\n",
      "  615/5000: episode: 23, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.135 [-0.756, 1.495], loss: 0.399448, mean_absolute_error: 1.963374, mean_q: 3.735474\n",
      "  626/5000: episode: 24, duration: 0.179s, episode steps: 11, steps per second: 62, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.121 [-0.775, 1.376], loss: 0.298114, mean_absolute_error: 1.938271, mean_q: 3.712403\n",
      "  640/5000: episode: 25, duration: 0.237s, episode steps: 14, steps per second: 59, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.077 [-0.807, 1.441], loss: 0.271502, mean_absolute_error: 1.994358, mean_q: 3.835075\n",
      "  650/5000: episode: 26, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.120 [-0.776, 1.493], loss: 0.238932, mean_absolute_error: 2.029151, mean_q: 3.972866\n",
      "  664/5000: episode: 27, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.077 [-0.994, 1.626], loss: 0.304859, mean_absolute_error: 2.130008, mean_q: 4.144618\n",
      "  676/5000: episode: 28, duration: 0.206s, episode steps: 12, steps per second: 58, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-0.842, 1.551], loss: 0.310429, mean_absolute_error: 2.152885, mean_q: 4.233344\n",
      "  688/5000: episode: 29, duration: 0.198s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.129 [-0.938, 1.665], loss: 0.472066, mean_absolute_error: 2.285398, mean_q: 4.350768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  700/5000: episode: 30, duration: 0.195s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.114 [-0.748, 1.469], loss: 0.418320, mean_absolute_error: 2.276746, mean_q: 4.351179\n",
      "  716/5000: episode: 31, duration: 0.275s, episode steps: 16, steps per second: 58, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.080 [-0.825, 1.483], loss: 0.448412, mean_absolute_error: 2.342621, mean_q: 4.463023\n",
      "  728/5000: episode: 32, duration: 0.191s, episode steps: 12, steps per second: 63, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.095 [-0.829, 1.288], loss: 0.527969, mean_absolute_error: 2.423585, mean_q: 4.571693\n",
      "  740/5000: episode: 33, duration: 0.197s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.109 [-0.765, 1.331], loss: 0.467757, mean_absolute_error: 2.443608, mean_q: 4.658665\n",
      "  752/5000: episode: 34, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.125 [-0.744, 1.522], loss: 0.583739, mean_absolute_error: 2.536068, mean_q: 4.779296\n",
      "  763/5000: episode: 35, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.108 [-0.814, 1.462], loss: 0.458946, mean_absolute_error: 2.535284, mean_q: 4.895838\n",
      "  777/5000: episode: 36, duration: 0.237s, episode steps: 14, steps per second: 59, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.092 [-0.815, 1.482], loss: 0.572627, mean_absolute_error: 2.598425, mean_q: 5.017778\n",
      "  791/5000: episode: 37, duration: 0.239s, episode steps: 14, steps per second: 59, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.098 [-0.961, 1.489], loss: 0.650100, mean_absolute_error: 2.696527, mean_q: 5.116138\n",
      "  801/5000: episode: 38, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-0.964, 1.701], loss: 0.601660, mean_absolute_error: 2.720572, mean_q: 5.126448\n",
      "  817/5000: episode: 39, duration: 0.265s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.091 [-0.785, 1.203], loss: 0.535644, mean_absolute_error: 2.738346, mean_q: 5.211985\n",
      "  829/5000: episode: 40, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.097 [-0.793, 1.173], loss: 0.450462, mean_absolute_error: 2.778513, mean_q: 5.354378\n",
      "  840/5000: episode: 41, duration: 0.188s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.131 [-0.597, 1.259], loss: 0.510584, mean_absolute_error: 2.854463, mean_q: 5.469688\n",
      "  853/5000: episode: 42, duration: 0.208s, episode steps: 13, steps per second: 63, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.092 [-0.829, 1.247], loss: 0.638730, mean_absolute_error: 2.900611, mean_q: 5.537694\n",
      "  863/5000: episode: 43, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.147 [-0.751, 1.418], loss: 0.597002, mean_absolute_error: 2.951509, mean_q: 5.654108\n",
      "  876/5000: episode: 44, duration: 0.214s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.112 [-0.750, 1.376], loss: 0.588146, mean_absolute_error: 2.971817, mean_q: 5.735247\n",
      "  886/5000: episode: 45, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.141 [-0.943, 1.674], loss: 0.601409, mean_absolute_error: 3.018384, mean_q: 5.878018\n",
      "  895/5000: episode: 46, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.154 [-0.949, 1.733], loss: 0.620369, mean_absolute_error: 3.100520, mean_q: 5.993104\n",
      "  905/5000: episode: 47, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-0.942, 1.619], loss: 0.581222, mean_absolute_error: 3.145679, mean_q: 6.009727\n",
      "  916/5000: episode: 48, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.091 [-1.014, 1.608], loss: 0.783223, mean_absolute_error: 3.182023, mean_q: 6.040096\n",
      "  928/5000: episode: 49, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-0.800, 1.490], loss: 1.071367, mean_absolute_error: 3.267329, mean_q: 6.084410\n",
      "  941/5000: episode: 50, duration: 0.237s, episode steps: 13, steps per second: 55, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.076 [-0.810, 1.345], loss: 0.871369, mean_absolute_error: 3.296885, mean_q: 6.077857\n",
      "  956/5000: episode: 51, duration: 0.249s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.083 [-0.752, 1.298], loss: 0.790159, mean_absolute_error: 3.277900, mean_q: 6.182659\n",
      "  975/5000: episode: 52, duration: 0.311s, episode steps: 19, steps per second: 61, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.060 [-0.814, 1.200], loss: 0.848476, mean_absolute_error: 3.350254, mean_q: 6.327723\n",
      "  990/5000: episode: 53, duration: 0.250s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.089 [-0.827, 1.300], loss: 1.094565, mean_absolute_error: 3.459780, mean_q: 6.407633\n",
      " 1005/5000: episode: 54, duration: 0.250s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.092 [-0.602, 1.153], loss: 0.786800, mean_absolute_error: 3.427517, mean_q: 6.466728\n",
      " 1018/5000: episode: 55, duration: 0.220s, episode steps: 13, steps per second: 59, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.102 [-0.773, 1.460], loss: 1.213815, mean_absolute_error: 3.562259, mean_q: 6.543820\n",
      " 1034/5000: episode: 56, duration: 0.261s, episode steps: 16, steps per second: 61, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.068 [-0.815, 1.288], loss: 0.680561, mean_absolute_error: 3.502790, mean_q: 6.616163\n",
      " 1048/5000: episode: 57, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.093 [-0.612, 1.189], loss: 0.969172, mean_absolute_error: 3.608564, mean_q: 6.822770\n",
      " 1064/5000: episode: 58, duration: 0.267s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.081 [-0.752, 1.189], loss: 0.945435, mean_absolute_error: 3.658339, mean_q: 6.886815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1076/5000: episode: 59, duration: 0.202s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.103 [-0.633, 1.139], loss: 1.085588, mean_absolute_error: 3.705820, mean_q: 6.959055\n",
      " 1090/5000: episode: 60, duration: 0.229s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.100 [-0.825, 1.234], loss: 0.959688, mean_absolute_error: 3.730137, mean_q: 6.978728\n",
      " 1103/5000: episode: 61, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.091 [-0.797, 1.250], loss: 1.188751, mean_absolute_error: 3.787336, mean_q: 7.086816\n",
      " 1117/5000: episode: 62, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.085 [-0.780, 1.285], loss: 1.125573, mean_absolute_error: 3.819210, mean_q: 7.125304\n",
      " 1136/5000: episode: 63, duration: 0.320s, episode steps: 19, steps per second: 59, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.078 [-0.738, 1.080], loss: 1.299346, mean_absolute_error: 3.874090, mean_q: 7.138863\n",
      " 1153/5000: episode: 64, duration: 0.279s, episode steps: 17, steps per second: 61, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.090 [-0.739, 1.175], loss: 1.235652, mean_absolute_error: 3.916286, mean_q: 7.235625\n",
      " 1173/5000: episode: 65, duration: 0.333s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.061 [-0.602, 1.036], loss: 1.392315, mean_absolute_error: 3.961184, mean_q: 7.280716\n",
      " 1191/5000: episode: 66, duration: 0.299s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.084 [-0.573, 1.027], loss: 1.009614, mean_absolute_error: 3.951282, mean_q: 7.376482\n",
      " 1210/5000: episode: 67, duration: 0.317s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.069 [-0.595, 0.987], loss: 1.019491, mean_absolute_error: 4.014131, mean_q: 7.605669\n",
      " 1225/5000: episode: 68, duration: 0.249s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.108 [-0.568, 1.020], loss: 1.373350, mean_absolute_error: 4.093890, mean_q: 7.604230\n",
      " 1242/5000: episode: 69, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.077 [-0.581, 1.068], loss: 1.138171, mean_absolute_error: 4.104575, mean_q: 7.627886\n",
      " 1261/5000: episode: 70, duration: 0.316s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.099 [-0.573, 0.944], loss: 1.041626, mean_absolute_error: 4.139230, mean_q: 7.797781\n",
      " 1277/5000: episode: 71, duration: 0.267s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.631, 0.906], loss: 1.374546, mean_absolute_error: 4.242967, mean_q: 7.861820\n",
      " 1294/5000: episode: 72, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.111 [-0.367, 0.803], loss: 1.587688, mean_absolute_error: 4.280374, mean_q: 7.805187\n",
      " 1322/5000: episode: 73, duration: 0.469s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.441, 0.815], loss: 1.164904, mean_absolute_error: 4.259941, mean_q: 7.958409\n",
      " 1342/5000: episode: 74, duration: 0.329s, episode steps: 20, steps per second: 61, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.362, 0.789], loss: 1.367061, mean_absolute_error: 4.361337, mean_q: 8.110401\n",
      " 1364/5000: episode: 75, duration: 0.366s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.567, 0.938], loss: 1.204517, mean_absolute_error: 4.369178, mean_q: 8.157375\n",
      " 1384/5000: episode: 76, duration: 0.336s, episode steps: 20, steps per second: 59, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.549, 1.042], loss: 1.133158, mean_absolute_error: 4.427874, mean_q: 8.398035\n",
      " 1411/5000: episode: 77, duration: 0.445s, episode steps: 27, steps per second: 61, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.069 [-0.560, 1.130], loss: 1.335167, mean_absolute_error: 4.496252, mean_q: 8.447364\n",
      " 1428/5000: episode: 78, duration: 0.284s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.115 [-0.548, 0.987], loss: 1.529545, mean_absolute_error: 4.561868, mean_q: 8.463248\n",
      " 1452/5000: episode: 79, duration: 0.399s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.362, 0.746], loss: 1.163683, mean_absolute_error: 4.553171, mean_q: 8.492664\n",
      " 1482/5000: episode: 80, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.101 [-0.544, 0.913], loss: 1.487581, mean_absolute_error: 4.641185, mean_q: 8.629856\n",
      " 1510/5000: episode: 81, duration: 0.468s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.102 [-0.386, 0.799], loss: 1.471968, mean_absolute_error: 4.709230, mean_q: 8.814744\n",
      " 1539/5000: episode: 82, duration: 0.481s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.080 [-0.353, 0.697], loss: 1.744446, mean_absolute_error: 4.794191, mean_q: 8.806396\n",
      " 1577/5000: episode: 83, duration: 0.633s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.102 [-0.414, 0.845], loss: 1.137234, mean_absolute_error: 4.776956, mean_q: 9.002374\n",
      " 1609/5000: episode: 84, duration: 0.533s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.105 [-0.347, 0.829], loss: 1.295132, mean_absolute_error: 4.894389, mean_q: 9.297504\n",
      " 1649/5000: episode: 85, duration: 0.667s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.131 [-0.400, 0.714], loss: 1.297055, mean_absolute_error: 4.982866, mean_q: 9.422121\n",
      " 1697/5000: episode: 86, duration: 0.800s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.142 [-0.379, 0.861], loss: 1.487794, mean_absolute_error: 5.100975, mean_q: 9.571231\n",
      " 1860/5000: episode: 87, duration: 2.723s, episode steps: 163, steps per second: 60, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.090 [-0.384, 0.951], loss: 1.459018, mean_absolute_error: 5.311721, mean_q: 10.046507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1912/5000: episode: 88, duration: 0.862s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.130 [-0.751, 0.248], loss: 1.334405, mean_absolute_error: 5.535321, mean_q: 10.569262\n",
      " 1951/5000: episode: 89, duration: 0.650s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.081 [-0.859, 0.283], loss: 1.157321, mean_absolute_error: 5.708306, mean_q: 11.112662\n",
      " 1979/5000: episode: 90, duration: 0.466s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.110 [-0.660, 0.433], loss: 1.602455, mean_absolute_error: 5.820862, mean_q: 11.133946\n",
      " 2019/5000: episode: 91, duration: 0.667s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.111 [-0.736, 0.413], loss: 1.794112, mean_absolute_error: 5.912345, mean_q: 11.231811\n",
      " 2045/5000: episode: 92, duration: 0.436s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-0.876, 0.367], loss: 1.285162, mean_absolute_error: 5.979702, mean_q: 11.548608\n",
      " 2063/5000: episode: 93, duration: 0.296s, episode steps: 18, steps per second: 61, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-0.998, 0.587], loss: 1.622865, mean_absolute_error: 6.110313, mean_q: 11.830317\n",
      " 2084/5000: episode: 94, duration: 0.350s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.098 [-0.821, 0.372], loss: 1.416287, mean_absolute_error: 6.134000, mean_q: 11.964262\n",
      " 2114/5000: episode: 95, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.040 [-0.844, 0.620], loss: 1.419542, mean_absolute_error: 6.231426, mean_q: 12.086204\n",
      " 2135/5000: episode: 96, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.078 [-1.107, 0.459], loss: 1.666302, mean_absolute_error: 6.282773, mean_q: 12.167006\n",
      " 2147/5000: episode: 97, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.111 [-1.078, 0.580], loss: 1.393629, mean_absolute_error: 6.415062, mean_q: 12.470600\n",
      " 2165/5000: episode: 98, duration: 0.298s, episode steps: 18, steps per second: 61, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.070 [-1.995, 1.210], loss: 2.194015, mean_absolute_error: 6.504031, mean_q: 12.511106\n",
      " 2175/5000: episode: 99, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.137 [-2.698, 1.749], loss: 2.057528, mean_absolute_error: 6.500045, mean_q: 12.457346\n",
      " 2184/5000: episode: 100, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.182 [-2.887, 1.722], loss: 1.767323, mean_absolute_error: 6.526216, mean_q: 12.625472\n",
      " 2199/5000: episode: 101, duration: 0.249s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.117 [-1.377, 0.613], loss: 3.442984, mean_absolute_error: 6.765643, mean_q: 12.733249\n",
      " 2211/5000: episode: 102, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.115 [-1.233, 0.588], loss: 2.011070, mean_absolute_error: 6.625231, mean_q: 12.537694\n",
      " 2219/5000: episode: 103, duration: 0.135s, episode steps: 8, steps per second: 59, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-2.541, 1.576], loss: 1.525302, mean_absolute_error: 6.672011, mean_q: 12.769411\n",
      " 2240/5000: episode: 104, duration: 0.346s, episode steps: 21, steps per second: 61, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.080 [-1.008, 0.587], loss: 1.872478, mean_absolute_error: 6.706927, mean_q: 12.945299\n",
      " 2253/5000: episode: 105, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.125 [-1.885, 0.975], loss: 1.638844, mean_absolute_error: 6.722788, mean_q: 13.014975\n",
      " 2267/5000: episode: 106, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.109 [-1.041, 0.583], loss: 3.031974, mean_absolute_error: 6.716366, mean_q: 12.871756\n",
      " 2282/5000: episode: 107, duration: 0.248s, episode steps: 15, steps per second: 61, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.083 [-0.971, 0.604], loss: 3.196194, mean_absolute_error: 7.018012, mean_q: 13.405390\n",
      " 2296/5000: episode: 108, duration: 0.235s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.077 [-2.063, 1.227], loss: 1.540423, mean_absolute_error: 6.776786, mean_q: 13.161589\n",
      " 2307/5000: episode: 109, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.121 [-2.708, 1.793], loss: 2.993363, mean_absolute_error: 7.049785, mean_q: 13.519774\n",
      " 2317/5000: episode: 110, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-3.113, 1.958], loss: 2.949103, mean_absolute_error: 7.078096, mean_q: 13.625949\n",
      " 2327/5000: episode: 111, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.145 [-2.533, 1.556], loss: 2.970860, mean_absolute_error: 7.086417, mean_q: 13.514795\n",
      " 2338/5000: episode: 112, duration: 0.186s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.125 [-2.873, 1.782], loss: 1.929580, mean_absolute_error: 6.974132, mean_q: 13.435978\n",
      " 2354/5000: episode: 113, duration: 0.263s, episode steps: 16, steps per second: 61, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.070 [-2.566, 1.594], loss: 2.504555, mean_absolute_error: 7.089163, mean_q: 13.649345\n",
      " 2383/5000: episode: 114, duration: 0.483s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.089 [-0.978, 0.550], loss: 2.694956, mean_absolute_error: 7.194366, mean_q: 13.660862\n",
      " 2413/5000: episode: 115, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-0.977, 0.372], loss: 2.622804, mean_absolute_error: 7.211253, mean_q: 13.778071\n",
      " 2468/5000: episode: 116, duration: 0.915s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.074 [-0.660, 0.361], loss: 3.552997, mean_absolute_error: 7.425404, mean_q: 14.025160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2516/5000: episode: 117, duration: 0.805s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.107 [-0.818, 0.431], loss: 2.954190, mean_absolute_error: 7.446457, mean_q: 14.125079\n",
      " 2552/5000: episode: 118, duration: 0.595s, episode steps: 36, steps per second: 61, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.124 [-0.783, 0.302], loss: 3.047535, mean_absolute_error: 7.500880, mean_q: 14.273122\n",
      " 2586/5000: episode: 119, duration: 0.566s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-0.970, 0.459], loss: 3.481608, mean_absolute_error: 7.633544, mean_q: 14.420965\n",
      " 2607/5000: episode: 120, duration: 0.350s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.098 [-1.003, 0.269], loss: 3.608931, mean_absolute_error: 7.745029, mean_q: 14.660711\n",
      " 2632/5000: episode: 121, duration: 0.421s, episode steps: 25, steps per second: 59, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.124 [-0.876, 0.357], loss: 2.713823, mean_absolute_error: 7.722135, mean_q: 14.734161\n",
      " 2698/5000: episode: 122, duration: 1.095s, episode steps: 66, steps per second: 60, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.129 [-0.407, 0.708], loss: 3.716076, mean_absolute_error: 7.819024, mean_q: 14.697264\n",
      " 2741/5000: episode: 123, duration: 0.717s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.093 [-1.068, 0.569], loss: 4.016618, mean_absolute_error: 7.900498, mean_q: 14.848889\n",
      " 2801/5000: episode: 124, duration: 1.000s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.127 [-0.252, 0.834], loss: 3.152418, mean_absolute_error: 7.897899, mean_q: 15.021921\n",
      " 2848/5000: episode: 125, duration: 0.783s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.103 [-0.222, 0.624], loss: 3.311556, mean_absolute_error: 7.951747, mean_q: 15.119177\n",
      " 2904/5000: episode: 126, duration: 0.934s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.071 [-0.554, 0.773], loss: 3.258506, mean_absolute_error: 8.013357, mean_q: 15.248103\n",
      " 2964/5000: episode: 127, duration: 1.000s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.089 [-0.353, 0.678], loss: 3.393990, mean_absolute_error: 8.103499, mean_q: 15.443334\n",
      " 3000/5000: episode: 128, duration: 0.603s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.137 [-0.156, 0.785], loss: 2.729640, mean_absolute_error: 8.069516, mean_q: 15.468687\n",
      " 3042/5000: episode: 129, duration: 0.696s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.129 [-0.217, 0.838], loss: 3.556756, mean_absolute_error: 8.204919, mean_q: 15.719349\n",
      " 3087/5000: episode: 130, duration: 0.750s, episode steps: 45, steps per second: 60, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.137 [-0.274, 0.702], loss: 2.923739, mean_absolute_error: 8.188771, mean_q: 15.659488\n",
      " 3194/5000: episode: 131, duration: 1.785s, episode steps: 107, steps per second: 60, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.094 [-0.424, 0.883], loss: 2.728428, mean_absolute_error: 8.312382, mean_q: 16.083857\n",
      " 3253/5000: episode: 132, duration: 0.983s, episode steps: 59, steps per second: 60, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.138 [-0.474, 0.741], loss: 3.139606, mean_absolute_error: 8.462163, mean_q: 16.277617\n",
      " 3321/5000: episode: 133, duration: 1.134s, episode steps: 68, steps per second: 60, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.146 [-0.257, 0.935], loss: 2.781388, mean_absolute_error: 8.544639, mean_q: 16.537151\n",
      " 3379/5000: episode: 134, duration: 0.967s, episode steps: 58, steps per second: 60, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.132 [-0.702, 0.413], loss: 2.657202, mean_absolute_error: 8.673486, mean_q: 16.876055\n",
      " 3444/5000: episode: 135, duration: 1.085s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.172 [-0.931, 0.456], loss: 3.337714, mean_absolute_error: 8.876891, mean_q: 17.210438\n",
      " 3532/5000: episode: 136, duration: 1.473s, episode steps: 88, steps per second: 60, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.102 [-0.764, 0.582], loss: 3.725161, mean_absolute_error: 9.053809, mean_q: 17.443031\n",
      " 3583/5000: episode: 137, duration: 0.844s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.131 [-0.904, 0.202], loss: 3.203204, mean_absolute_error: 9.175821, mean_q: 17.807968\n",
      " 3654/5000: episode: 138, duration: 1.188s, episode steps: 71, steps per second: 60, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.098 [-0.714, 0.334], loss: 3.356858, mean_absolute_error: 9.387556, mean_q: 18.196806\n",
      " 3714/5000: episode: 139, duration: 1.002s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.088 [-0.986, 0.425], loss: 4.327169, mean_absolute_error: 9.501102, mean_q: 18.291206\n",
      " 3758/5000: episode: 140, duration: 0.726s, episode steps: 44, steps per second: 61, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.127 [-0.718, 0.562], loss: 4.860805, mean_absolute_error: 9.619675, mean_q: 18.462688\n",
      " 3827/5000: episode: 141, duration: 1.148s, episode steps: 69, steps per second: 60, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.106 [-0.860, 0.370], loss: 3.063923, mean_absolute_error: 9.685150, mean_q: 18.802420\n",
      " 3884/5000: episode: 142, duration: 0.951s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.150 [-0.734, 0.341], loss: 2.525238, mean_absolute_error: 9.776417, mean_q: 19.104147\n",
      " 3959/5000: episode: 143, duration: 1.249s, episode steps: 75, steps per second: 60, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.200 [-0.310, 0.945], loss: 4.053843, mean_absolute_error: 9.951249, mean_q: 19.264311\n",
      " 4068/5000: episode: 144, duration: 1.820s, episode steps: 109, steps per second: 60, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.121 [-0.928, 0.531], loss: 4.030017, mean_absolute_error: 10.081555, mean_q: 19.505829\n",
      " 4117/5000: episode: 145, duration: 0.815s, episode steps: 49, steps per second: 60, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.165 [-0.371, 0.927], loss: 3.584491, mean_absolute_error: 10.184094, mean_q: 19.704376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4183/5000: episode: 146, duration: 1.237s, episode steps: 66, steps per second: 53, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.159 [-0.761, 0.490], loss: 4.870924, mean_absolute_error: 10.359591, mean_q: 19.968761\n",
      " 4261/5000: episode: 147, duration: 1.430s, episode steps: 78, steps per second: 55, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.123 [-0.753, 0.262], loss: 4.205443, mean_absolute_error: 10.393514, mean_q: 20.113487\n",
      " 4461/5000: episode: 148, duration: 3.720s, episode steps: 200, steps per second: 54, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.056 [-0.758, 0.731], loss: 4.566591, mean_absolute_error: 10.656670, mean_q: 20.602295\n",
      " 4535/5000: episode: 149, duration: 1.233s, episode steps: 74, steps per second: 60, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.132 [-1.182, 0.347], loss: 4.474833, mean_absolute_error: 10.785823, mean_q: 20.835407\n",
      " 4583/5000: episode: 150, duration: 0.800s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.136 [-0.734, 0.260], loss: 5.125930, mean_absolute_error: 10.929054, mean_q: 21.136141\n",
      " 4645/5000: episode: 151, duration: 1.100s, episode steps: 62, steps per second: 56, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.134 [-0.918, 0.214], loss: 4.118124, mean_absolute_error: 10.966674, mean_q: 21.305368\n",
      " 4757/5000: episode: 152, duration: 1.968s, episode steps: 112, steps per second: 57, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.135 [-1.298, 0.379], loss: 4.533782, mean_absolute_error: 11.193904, mean_q: 21.676991\n",
      " 4831/5000: episode: 153, duration: 1.234s, episode steps: 74, steps per second: 60, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.159 [-0.291, 1.092], loss: 3.977381, mean_absolute_error: 11.197537, mean_q: 21.764515\n",
      " 4955/5000: episode: 154, duration: 2.119s, episode steps: 124, steps per second: 59, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.088 [-0.746, 0.482], loss: 4.400182, mean_absolute_error: 11.432844, mean_q: 22.251549\n",
      "done, took 93.973 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21bac365748>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 197.000, steps: 197\n",
      "Episode 2: reward: 76.000, steps: 76\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 78.000, steps: 78\n",
      "Episode 5: reward: 109.000, steps: 109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21babfcb630>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
