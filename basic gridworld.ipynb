{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training agent...\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'epsilon_decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-611996d815ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;31m# Driver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-611996d815ad>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# Decay agent exploration parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Print\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'epsilon_decay'"
     ]
    }
   ],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, Ny=8, Nx=8):\n",
    "        #define state space\n",
    "        self.Ny = Ny # y grid size\n",
    "        self.Nx = Nx #x grid size\n",
    "        self.state_dim = (Ny, Nx)\n",
    "        #define action space\n",
    "        self.action_dim = (4,) #up, right, down, left\n",
    "        self.action_dict = {\"up\": 0, \"right\": 1, \"down\": 2, \"left\": 3}\n",
    "        self.action_coords = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        #define reward table\n",
    "        self.R = self._build_rewards() #R(s,a) agent rewards\n",
    "        #check action space consistency\n",
    "        if len(self.action_dict.keys()) != len(self.action_coords):\n",
    "            exit(\"err: inconsistent actions given\")\n",
    "            \n",
    "    def reset(self):\n",
    "        #Reset agent state to top left grid corner\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        #evolve agent state\n",
    "        state_next = (self.state[0] + self.action_coords[action][0],\n",
    "                      self.state[1] + self.action_coords[action][1])\n",
    "        #collect reward\n",
    "        reward = self.R[self.state + (action,)]\n",
    "        #terminate if we reach bottom right grid corner\n",
    "        done = (state_next[0] == self.Ny - 1) and (state_next[1] == self.Nx - 1)\n",
    "        \n",
    "        #update state\n",
    "        self.state = state_next\n",
    "        return state_next, reward, done\n",
    "    \n",
    "    def allowed_actions(self):\n",
    "        #Generate list of actions allowed depending on agent grid location\n",
    "        actions_allowed = []\n",
    "        y, x = self.state[0], self.state[1]\n",
    "        if(y>0): #no passing top boundary\n",
    "            actions_allowed.append(self.action_dict[\"up\"])\n",
    "        if(y<self.Ny - 1): # no passing bottom boundary\n",
    "            actions_allowed.append(self.action_dict[\"down\"])\n",
    "        if(x>0):# no passing left boundary\n",
    "            actions_allowed.append(self.action_dict[\"left\"])\n",
    "        if(x<self.Nx - 1): #no passing right boundary\n",
    "            actions_allowed.append(self.action_dict[\"right\"])\n",
    "        actions_allowed = np.array(actions_allowed, dtype=int)\n",
    "        return actions_allowed\n",
    "        \n",
    "    def _build_rewards(self):\n",
    "        r_goal = 100 #reward for arriving at terminal state (bottom right corner)\n",
    "        r_nongoal = -0.1 #penalty for not reaching terminal state\n",
    "        R = r_nongoal * np.ones(self.state_dim + self.action_dim, dtype=float) #R[s, a]\n",
    "        R[self.Ny -2, self.Nx -1, self.action_dict[\"down\"]] = r_goal # arrive from above\n",
    "        R[self.Ny - 1, self.Nx -2, self.action_dict[\"right\"]] = r_goal #arrive from the left\n",
    "        return R\n",
    " \n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.state_dim = env.state_dim\n",
    "        self.action_dim = env.action_dim\n",
    "        #Agent learning parameters\n",
    "        self.epsilon = 1.0\n",
    "        self_epsilon_decay = 0.99 #epsilon decay after each episode\n",
    "        self.beta = 0.99 #learning rate\n",
    "        self.gamma = 0.99 #reward discount factor\n",
    "        #initialize Q[s,a] table\n",
    "        self.Q = np.zeros(self.state_dim + self.action_dim, dtype=float)\n",
    "        \n",
    "    def get_action(self, env):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(env.allowed_actions())\n",
    "        else:\n",
    "            state = env.state;\n",
    "            actions_allowed = env.allowed_actions()\n",
    "            Q_s = self.Q[state[0], state[1], actions_allowed]\n",
    "            actions_greedy = actions_allowed[np.flatnonzero(Q_s == np.max(Q_s))]\n",
    "            return np.random.choice(actions_greedy)\n",
    "        \n",
    "    def train(self, memory):\n",
    "        (state, action, state_next, reward, done) = memory\n",
    "        sa = state + (action,)\n",
    "        self.Q[sa] += self.beta * (reward + self.gamma*np.max(self.Q[state_next]) - self.Q[sa])\n",
    "        \n",
    "    def display_greedy_policy(self):\n",
    "        greedy_policy = np.zeros((self.state_dim[0], self.state_dim[1]), dtype=int)\n",
    "        for x in range(self.state_dim[0]):\n",
    "            for y in range(self.state_dim[1]):\n",
    "                greedy_policy[y, x] = np.argmax(self.Q[y, x, :])\n",
    "        print(\"\\nGreedy policy(y, x):\")\n",
    "        print(greedy_policy)\n",
    "        print()\n",
    "        \n",
    "def main():\n",
    "\n",
    "    # Settings\n",
    "    env = Environment(Ny=8, Nx=8) \n",
    "    agent = Agent(env)\n",
    "\n",
    "    # Train agent\n",
    "    print(\"\\nTraining agent...\\n\")\n",
    "    N_episodes = 500\n",
    "    for episode in range(N_episodes):\n",
    "\n",
    "        # Generate an episode\n",
    "        iter_episode, reward_episode = 0, 0  \n",
    "        state = env.reset()  # starting state\n",
    "        while True:\n",
    "            action = agent.get_action(env)  # get action\n",
    "            state_next, reward, done = env.step(action)  # evolve state by action\n",
    "            agent.train((state, action, state_next, reward, done))  # train agent\n",
    "            iter_episode += 1\n",
    "            reward_episode += reward \n",
    "            if done:\n",
    "                break\n",
    "            state = state_next  # transition to next state\n",
    "\n",
    "        # Decay agent exploration parameter\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, 0.01)\n",
    "\n",
    "        # Print\n",
    "        if (episode == 0) or (episode + 1) % 10 == 0: \n",
    "            print(\"[episode {}/{}] eps = {:.3F} -> iter = {}, rew = {:.1F}\".format(\n",
    "                episode + 1, N_episodes, agent.epsilon, iter_episode, reward_episode))\n",
    "\n",
    "        # Print greedy policy\n",
    "        if (episode == N_episodes - 1):\n",
    "            agent.display_greedy_policy()\n",
    "            for (key, val) in sorted(env.action_dict.items(), key=operator.itemgetter(1)):\n",
    "                print(\" action['{}'] = {}\".format(key, val))\n",
    "            print()\n",
    "\n",
    "# Driver\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_greedy_policy(self):\n",
    "        # greedy policy = argmax[a'] Q[s,a']\n",
    "        greedy_policy = np.zeros((self.state_dim[0], self.state_dim[1]), dtype=int)\n",
    "        for x in range(self.state_dim[0]):\n",
    "            for y in range(self.state_dim[1]):\n",
    "                greedy_policy[y, x] = np.argmax(self.Q[y, x, :])\n",
    "        print(\"\\nGreedy policy(y, x):\")\n",
    "        print(greedy_policy)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9cf18f1ed731>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maction_coords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# translations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Define rewards table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_coords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'action' is not defined"
     ]
    }
   ],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training agent...\n",
      "\n",
      "[episode 1/500] eps = 0.990 -> iter = 50, rew = 95.1\n",
      "[episode 10/500] eps = 0.904 -> iter = 270, rew = 73.1\n",
      "[episode 20/500] eps = 0.818 -> iter = 54, rew = 94.7\n",
      "[episode 30/500] eps = 0.740 -> iter = 84, rew = 91.7\n",
      "[episode 40/500] eps = 0.669 -> iter = 64, rew = 93.7\n",
      "[episode 50/500] eps = 0.605 -> iter = 36, rew = 96.5\n",
      "[episode 60/500] eps = 0.547 -> iter = 80, rew = 92.1\n",
      "[episode 70/500] eps = 0.495 -> iter = 16, rew = 98.5\n",
      "[episode 80/500] eps = 0.448 -> iter = 26, rew = 97.5\n",
      "[episode 90/500] eps = 0.405 -> iter = 20, rew = 98.1\n",
      "[episode 100/500] eps = 0.366 -> iter = 20, rew = 98.1\n",
      "[episode 110/500] eps = 0.331 -> iter = 18, rew = 98.3\n",
      "[episode 120/500] eps = 0.299 -> iter = 22, rew = 97.9\n",
      "[episode 130/500] eps = 0.271 -> iter = 14, rew = 98.7\n",
      "[episode 140/500] eps = 0.245 -> iter = 14, rew = 98.7\n",
      "[episode 150/500] eps = 0.221 -> iter = 16, rew = 98.5\n",
      "[episode 160/500] eps = 0.200 -> iter = 16, rew = 98.5\n",
      "[episode 170/500] eps = 0.181 -> iter = 16, rew = 98.5\n",
      "[episode 180/500] eps = 0.164 -> iter = 16, rew = 98.5\n",
      "[episode 190/500] eps = 0.148 -> iter = 14, rew = 98.7\n",
      "[episode 200/500] eps = 0.134 -> iter = 18, rew = 98.3\n",
      "[episode 210/500] eps = 0.121 -> iter = 14, rew = 98.7\n",
      "[episode 220/500] eps = 0.110 -> iter = 14, rew = 98.7\n",
      "[episode 230/500] eps = 0.099 -> iter = 14, rew = 98.7\n",
      "[episode 240/500] eps = 0.090 -> iter = 14, rew = 98.7\n",
      "[episode 250/500] eps = 0.081 -> iter = 14, rew = 98.7\n",
      "[episode 260/500] eps = 0.073 -> iter = 14, rew = 98.7\n",
      "[episode 270/500] eps = 0.066 -> iter = 14, rew = 98.7\n",
      "[episode 280/500] eps = 0.060 -> iter = 16, rew = 98.5\n",
      "[episode 290/500] eps = 0.054 -> iter = 16, rew = 98.5\n",
      "[episode 300/500] eps = 0.049 -> iter = 18, rew = 98.3\n",
      "[episode 310/500] eps = 0.044 -> iter = 14, rew = 98.7\n",
      "[episode 320/500] eps = 0.040 -> iter = 14, rew = 98.7\n",
      "[episode 330/500] eps = 0.036 -> iter = 14, rew = 98.7\n",
      "[episode 340/500] eps = 0.033 -> iter = 14, rew = 98.7\n",
      "[episode 350/500] eps = 0.030 -> iter = 14, rew = 98.7\n",
      "[episode 360/500] eps = 0.027 -> iter = 14, rew = 98.7\n",
      "[episode 370/500] eps = 0.024 -> iter = 14, rew = 98.7\n",
      "[episode 380/500] eps = 0.022 -> iter = 16, rew = 98.5\n",
      "[episode 390/500] eps = 0.020 -> iter = 14, rew = 98.7\n",
      "[episode 400/500] eps = 0.018 -> iter = 16, rew = 98.5\n",
      "[episode 410/500] eps = 0.016 -> iter = 14, rew = 98.7\n",
      "[episode 420/500] eps = 0.015 -> iter = 14, rew = 98.7\n",
      "[episode 430/500] eps = 0.013 -> iter = 14, rew = 98.7\n",
      "[episode 440/500] eps = 0.012 -> iter = 14, rew = 98.7\n",
      "[episode 450/500] eps = 0.011 -> iter = 14, rew = 98.7\n",
      "[episode 460/500] eps = 0.010 -> iter = 14, rew = 98.7\n",
      "[episode 470/500] eps = 0.010 -> iter = 14, rew = 98.7\n",
      "[episode 480/500] eps = 0.010 -> iter = 14, rew = 98.7\n",
      "[episode 490/500] eps = 0.010 -> iter = 16, rew = 98.5\n",
      "[episode 500/500] eps = 0.010 -> iter = 14, rew = 98.7\n",
      "\n",
      "Greedy policy(y, x):\n",
      "[[1 1 1 2 2 2 2 2]\n",
      " [1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 0]]\n",
      "\n",
      " action['up'] = 0\n",
      " action['right'] = 1\n",
      " action['down'] = 2\n",
      " action['left'] = 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys, random, operator\n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    \n",
    "    def __init__(self, Ny=8, Nx=8):\n",
    "        # Define state space\n",
    "        self.Ny = Ny  # y grid size\n",
    "        self.Nx = Nx  # x grid size\n",
    "        self.state_dim = (Ny, Nx)\n",
    "        # Define action space\n",
    "        self.action_dim = (4,)  # up, right, down, left\n",
    "        self.action_dict = {\"up\": 0, \"right\": 1, \"down\": 2, \"left\": 3}\n",
    "        self.action_coords = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # translations\n",
    "        # Define rewards table\n",
    "        self.R = self._build_rewards()  # R(s,a) agent rewards\n",
    "        # Check action space consistency\n",
    "        if len(self.action_dict.keys()) != len(self.action_coords):\n",
    "            exit(\"err: inconsistent actions given\")\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agent state to top-left grid corner\n",
    "        self.state = (0, 0)  \n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Evolve agent state\n",
    "        state_next = (self.state[0] + self.action_coords[action][0],\n",
    "                      self.state[1] + self.action_coords[action][1])\n",
    "        # Collect reward\n",
    "        reward = self.R[self.state + (action,)]\n",
    "        # Terminate if we reach bottom-right grid corner\n",
    "        done = (state_next[0] == self.Ny - 1) and (state_next[1] == self.Nx - 1)\n",
    "        # Update state\n",
    "        self.state = state_next\n",
    "        return state_next, reward, done\n",
    "    \n",
    "    def allowed_actions(self):\n",
    "        # Generate list of actions allowed depending on agent grid location\n",
    "        actions_allowed = []\n",
    "        y, x = self.state[0], self.state[1]\n",
    "        if (y > 0):  # no passing top-boundary\n",
    "            actions_allowed.append(self.action_dict[\"up\"])\n",
    "        if (y < self.Ny - 1):  # no passing bottom-boundary\n",
    "            actions_allowed.append(self.action_dict[\"down\"])\n",
    "        if (x > 0):  # no passing left-boundary\n",
    "            actions_allowed.append(self.action_dict[\"left\"])\n",
    "        if (x < self.Nx - 1):  # no passing right-boundary\n",
    "            actions_allowed.append(self.action_dict[\"right\"])\n",
    "        actions_allowed = np.array(actions_allowed, dtype=int)\n",
    "        return actions_allowed\n",
    "\n",
    "    def _build_rewards(self):\n",
    "        # Define agent rewards R[s,a]\n",
    "        r_goal = 100  # reward for arriving at terminal state (bottom-right corner)\n",
    "        r_nongoal = -0.1  # penalty for not reaching terminal state\n",
    "        R = r_nongoal * np.ones(self.state_dim + self.action_dim, dtype=float)  # R[s,a]\n",
    "        R[self.Ny - 2, self.Nx - 1, self.action_dict[\"down\"]] = r_goal  # arrive from above\n",
    "        R[self.Ny - 1, self.Nx - 2, self.action_dict[\"right\"]] = r_goal  # arrive from the left\n",
    "        return R\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        # Store state and action dimension \n",
    "        self.state_dim = env.state_dim\n",
    "        self.action_dim = env.action_dim\n",
    "        # Agent learning parameters\n",
    "        self.epsilon = 1.0  # initial exploration probability\n",
    "        self.epsilon_decay = 0.99  # epsilon decay after each episode\n",
    "        self.beta = 0.99  # learning rate\n",
    "        self.gamma = 0.99  # reward discount factor\n",
    "        # Initialize Q[s,a] table\n",
    "        self.Q = np.zeros(self.state_dim + self.action_dim, dtype=float)\n",
    "\n",
    "    def get_action(self, env):\n",
    "        # Epsilon-greedy agent policy\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            # explore\n",
    "            return np.random.choice(env.allowed_actions())\n",
    "        else:\n",
    "            # exploit on allowed actions\n",
    "            state = env.state;\n",
    "            actions_allowed = env.allowed_actions()\n",
    "            Q_s = self.Q[state[0], state[1], actions_allowed]\n",
    "            actions_greedy = actions_allowed[np.flatnonzero(Q_s == np.max(Q_s))]\n",
    "            return np.random.choice(actions_greedy)\n",
    "\n",
    "    def train(self, memory):\n",
    "        # -----------------------------\n",
    "        # Update:\n",
    "        #\n",
    "        # Q[s,a] <- Q[s,a] + beta * (R[s,a] + gamma * max(Q[s,:]) - Q[s,a])\n",
    "        #\n",
    "        #  R[s,a] = reward for taking action a from state s\n",
    "        #  beta = learning rate\n",
    "        #  gamma = discount factor\n",
    "        # -----------------------------\n",
    "        (state, action, state_next, reward, done) = memory\n",
    "        sa = state + (action,)\n",
    "        self.Q[sa] += self.beta * (reward + self.gamma*np.max(self.Q[state_next]) - self.Q[sa])\n",
    "\n",
    "    def display_greedy_policy(self):\n",
    "        # greedy policy = argmax[a'] Q[s,a']\n",
    "        greedy_policy = np.zeros((self.state_dim[0], self.state_dim[1]), dtype=int)\n",
    "        for x in range(self.state_dim[0]):\n",
    "            for y in range(self.state_dim[1]):\n",
    "                greedy_policy[y, x] = np.argmax(self.Q[y, x, :])\n",
    "        print(\"\\nGreedy policy(y, x):\")\n",
    "        print(greedy_policy)\n",
    "        print()\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Settings\n",
    "    env = Environment(Ny=8, Nx=8) \n",
    "    agent = Agent(env)\n",
    "\n",
    "    # Train agent\n",
    "    print(\"\\nTraining agent...\\n\")\n",
    "    N_episodes = 500\n",
    "    for episode in range(N_episodes):\n",
    "\n",
    "        # Generate an episode\n",
    "        iter_episode, reward_episode = 0, 0  \n",
    "        state = env.reset()  # starting state\n",
    "        while True:\n",
    "            action = agent.get_action(env)  # get action\n",
    "            state_next, reward, done = env.step(action)  # evolve state by action\n",
    "            agent.train((state, action, state_next, reward, done))  # train agent\n",
    "            iter_episode += 1\n",
    "            reward_episode += reward \n",
    "            if done:\n",
    "                break\n",
    "            state = state_next  # transition to next state\n",
    "\n",
    "        # Decay agent exploration parameter\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, 0.01)\n",
    "\n",
    "        # Print\n",
    "        if (episode == 0) or (episode + 1) % 10 == 0: \n",
    "            print(\"[episode {}/{}] eps = {:.3F} -> iter = {}, rew = {:.1F}\".format(\n",
    "                episode + 1, N_episodes, agent.epsilon, iter_episode, reward_episode))\n",
    "\n",
    "        # Print greedy policy\n",
    "        if (episode == N_episodes - 1):\n",
    "            agent.display_greedy_policy()\n",
    "            for (key, val) in sorted(env.action_dict.items(), key=operator.itemgetter(1)):\n",
    "                print(\" action['{}'] = {}\".format(key, val))\n",
    "            print()\n",
    "\n",
    "# Driver\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
